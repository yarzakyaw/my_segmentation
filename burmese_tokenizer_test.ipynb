{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8b925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import build_context_aware_frequency_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646ca8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokenized corpus: 100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example initial dictionary\n",
    "    initial_dictionary_path = \"initial_dictionary.json\"\n",
    "\n",
    "    with open(initial_dictionary_path, 'r', encoding='utf-8') as f:\n",
    "        initial_dictionary = json.load(f)\n",
    "\n",
    "    # Example pre-tokenized corpus\n",
    "    tokenized_corpus = [\n",
    "        [\"ပညာရေး\", \"ဝန်ကြီးဌာန\", \"သည်\", \"အထက်တန်း\", \"ကျောင်း\", \"များ\", \"တွင်\", \"သင်ကြား\", \"ရေး\", \"အထောက်အကူ\", \"ပြု\", \"စာအုပ်\", \"ကို\", \"ထုတ်ဝေ\", \"ခဲ့\", \"။\"],\n",
    "        [\"ဗိုလ်ချုပ်ကတော်\", \"တော်လှန်\", \"ရေး\", \"အကြောင်း\", \"စာအုပ်\", \"ထုတ်ဝေ\", \"သည်\", \"။\"]\n",
    "    ]\n",
    "\n",
    "    # Build context-aware dictionary\n",
    "    context_aware_dict = build_context_aware_frequency_dictionary(\n",
    "        tokenized_corpus=tokenized_corpus,\n",
    "        initial_dictionary=initial_dictionary,\n",
    "        top_k=3\n",
    "    )\n",
    "\n",
    "    # print(\"Context-aware dictionary:\", json.dumps(context_aware_dict, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70920ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (no BPE, text1): ['ပညာရေး', 'ဝန်ကြီး', 'ဌာန', 'သည်', ' ', 'အထက်တန်း', 'ကျောင်း', 'များ', 'တွင်', ' ', 'သင်ကြား', 'မှု', 'အတွက်', ' ', 'သင်ကြား', 'ရေး', 'အထောက်အကူ', 'ပြု', ' ', 'စာအုပ်', 'များ', 'ကို', ' ', 'ထုတ်ဝေ', 'ခဲ့', 'သည်', '။']\n",
      "Token IDs (no BPE, text1): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "Tokens (no BPE, text2): ['တော်လှန်', 'ရေး', 'အထောက်အကူ', 'အတွက်', 'သင်ကြား', 'သည်', '။']\n",
      "Token IDs (no BPE, text2): [0, 1, 2, 3, 4, 5, 6]\n",
      "Tokens (with BPE, text1): ['ပညာရေး', 'ဝန်ကြီး', 'ဌာန', 'သည်', 'အထက်တန်း', 'ကျောင်း', 'များ', 'တွင်', 'သင်ကြား', 'မှု', 'အတွက်', 'သင်ကြား', 'ရေး', 'အထောက်အကူ', 'ပြု', 'စာအုပ်', 'များ', 'ကို', 'ထုတ်ဝေ', 'ခဲ့', 'သည်', '။']\n",
      "Token IDs (with BPE, text1): [14, 18, 17, 12, 19, 20, 22, 23, 27, 125, 24, 27, 28, 29, 30, 31, 22, 10, 32, 33, 12, 34]\n",
      "Tokens (with BPE, text2): ['တော်လှန်', 'ရေး', 'အထောက်အကူ', 'အတွက်', 'သင်ကြား', 'သည်', '။']\n",
      "Token IDs (with BPE, text2): [37, 28, 29, 24, 27, 12, 34]\n"
     ]
    }
   ],
   "source": [
    "from BurmeseTokenizer import BurmeseTokenizer\n",
    "# Initialize tokenizer with context-aware dictionary\n",
    "tokenizer = BurmeseTokenizer(context_aware_dict_path=\"context_aware_dictionary.json\")\n",
    "\n",
    "# Test context-aware maximum matching\n",
    "# test_text = \"ပညာရေးဝန်ကြီးဌာနသည်\"\n",
    "# syllables = tokenizer.segment_syllables(test_text)\n",
    "# tokens = tokenizer.context_aware_maximum_matching(syllables, use_context=True)\n",
    "# print(\"Context-aware tokens:\", tokens)\n",
    "\n",
    "# Example texts\n",
    "text1 = \"ပညာရေးဝန်ကြီးဌာနသည် အထက်တန်းကျောင်းများတွင် သင်ကြားမှုအတွက် သင်ကြားရေးအထောက်အကူပြု စာအုပ်များကို ထုတ်ဝေခဲ့သည်။\"\n",
    "text2 = \"တော်လှန်ရေးအထောက်အကူအတွက်သင်ကြားသည်။\"\n",
    "text3 = \"ဗိုလ်ချုပ်ကတော်လှန်သည်\"\n",
    "\n",
    "# Tokenize without BPE\n",
    "tokens_1, token_ids_1 = tokenizer.tokenize(text1, use_bpe=False)\n",
    "print(\"Tokens (no BPE, text1):\", tokens_1)\n",
    "print(\"Token IDs (no BPE, text1):\", token_ids_1)\n",
    "\n",
    "# Tokenize without BPE\n",
    "tokens_2, token_ids_2 = tokenizer.tokenize(text2, use_bpe=False)\n",
    "print(\"Tokens (no BPE, text2):\", tokens_2)\n",
    "print(\"Token IDs (no BPE, text2):\", token_ids_2)\n",
    "\n",
    "# Train BPE on your raw Burmese text corpus\n",
    "corpus = [text1, text2] * 50  # Replace with your actual raw text corpus\n",
    "tokenizer.train_bpe(corpus)\n",
    "\n",
    "# Tokenize with BPE\n",
    "tokens_1_BPE, token_ids_1_BPE = tokenizer.tokenize(text1, use_bpe=True)\n",
    "print(\"Tokens (with BPE, text1):\", tokens_1_BPE)\n",
    "print(\"Token IDs (with BPE, text1):\", token_ids_1_BPE)\n",
    "\n",
    "# Tokenize with BPE\n",
    "tokens_2_BPE, token_ids_2_BPE = tokenizer.tokenize(text2, use_bpe=True)\n",
    "print(\"Tokens (with BPE, text2):\", tokens_2_BPE)\n",
    "print(\"Token IDs (with BPE, text2):\", token_ids_2_BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e363c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
