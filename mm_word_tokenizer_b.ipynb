{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaba087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from typing import List, Dict, Tuple\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0109999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syllable(label: str, burmese_consonant: str, others: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Segment a Burmese word into syllables using regex-based rules.\n",
    "    \n",
    "    Args:\n",
    "        label (str): Input Burmese text or word.\n",
    "        burmese_consonant (str): Regex range for Burmese consonants.\n",
    "        others (str): Regex range for other characters (vowels, punctuation, etc.).\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of syllables.\n",
    "    \"\"\"\n",
    "    # Define regex patterns for Burmese consonants and other characters\n",
    "    # label = re.sub(r\"(?<![္])([\"+burmese_consonant+\"])(?![်္|့])|([\"+others+\"])\", r\" \\1\\2\", label).strip()\n",
    "    # label = re.sub('(?<=[က-ၴ])([a-zA-Z0-9])', r' \\1', label)\n",
    "    # label = re.sub('([0-9၀-၉])\\s+([0-9၀-၉])\\s*', r'\\1\\2 ', label)\n",
    "    # label = re.sub('([0-9၀-၉])\\s+(\\+)', r'\\1 \\2 ', label)\n",
    "    # label = label.split()\n",
    "    label = re.sub(r\"(?<![္])([\" + burmese_consonant + r\"])(?![်္|့])|([\" + others + r\"])\", r\" \\1\\2\", label).strip()\n",
    "    label = re.sub(r\"(?<=[က-ၴ])([a-zA-Z0-9])\", r\" \\1\", label)\n",
    "    label = re.sub(r\"([0-9၀-၉])\\s+([0-9၀-၉])\\s*\", r\"\\1\\2 \", label)\n",
    "    label = re.sub(r\"([0-9၀-၉])\\s+(\\+)\", r\"\\1 \\2 \", label)\n",
    "    label = label.split()\n",
    "    \n",
    "    return label\n",
    "\n",
    "def syllable_split(label: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split Burmese text into syllables, handling spaces and word boundaries.\n",
    "    \n",
    "    Args:\n",
    "        label (str): Input Burmese text.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of syllables.\n",
    "    \"\"\"\n",
    "    burmese_consonant = 'က-အ'\n",
    "    others = r\"ဣဤဥဦဧဩဪဿ၌၍၏၀-၉၊။!-/:-@[-`{-~\\s.,\"\n",
    "    \n",
    "    label_syllable = [get_syllable(s, burmese_consonant, others) + [' '] for s in label.split()]\n",
    "    return [s for sublist in label_syllable for s in sublist][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1927d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trie implementation for efficient dictionary lookups\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end = False\n",
    "        self.token = None\n",
    "        self.freq = 0.0  # Frequency for scoring\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, word: str, freq: float = 1.0):\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.is_end = True\n",
    "        node.token = word\n",
    "        node.freq = freq\n",
    "        # print(f\"Inserted token: {word}, Frequency: {freq}\")\n",
    "\n",
    "    def find_all_matches(self, syllables: List[str], start: int) -> List[Tuple[str, float, int]]:\n",
    "        matches = []\n",
    "        max_len = min(10, len(syllables) - start)  # Limit max token length (adjust as needed)\n",
    "        \n",
    "        for length in range(1, max_len + 1):\n",
    "            candidate = ''.join(syllables[start:start + length])\n",
    "            node = self.root\n",
    "            valid = True\n",
    "            # Traverse trie character by character\n",
    "            for char in candidate:\n",
    "                if char not in node.children:\n",
    "                    valid = False\n",
    "                    break\n",
    "                node = node.children[char]\n",
    "            if valid and node.is_end:\n",
    "                matches.append((node.token, node.freq, start + length))\n",
    "        return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ece18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (no BPE, text1): ['ပညာ', 'ရေး', 'ဝန်ကြီးဌာန', 'သည်', ' ', 'အထက်တန်း', 'ကျောင်း', 'များ', 'တွင်', ' ', 'သင်ကြား', 'မှု', 'အတွက်', ' ', 'သင်ကြား', 'ရေး', 'အထောက်အကူ', 'ပြု', ' ', 'စာအုပ်', 'များ', 'ကို', ' ', 'ထုတ်ဝေ', 'ခဲ့', 'သည်', '။']\n",
      "Token IDs (no BPE, text1): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "Tokens (no BPE, text2): ['တော်လှန်', 'ရေး', 'အထောက်အကူ', 'အတွက်', 'သင်ကြား', 'သည်', '။']\n",
      "Token IDs (no BPE, text2): [0, 1, 2, 3, 4, 5, 6]\n",
      "Tokens (with BPE, text1): ['ပညာ', 'ရေး', 'ဝန်ကြီးဌာန', 'သည်', 'အထက်တန်း', 'ကျောင်း', 'များ', 'တွင်', 'သင်ကြား', 'မှု', 'အတွက်', 'သင်ကြား', 'ရေး', 'အထောက်အကူ', 'ပြု', 'စာအုပ်', 'များ', 'ကို', 'ထုတ်ဝေ', 'ခဲ့', 'သည်', '။']\n",
      "Token IDs (with BPE, text1): [1, 11, 2, 4, 5, 7, 8, 9, 10, 109, 12, 10, 11, 13, 14, 15, 8, 16, 17, 20, 4, 21]\n",
      "Tokens (with BPE, text2): ['တော်လှန်', 'ရေး', 'အထောက်အကူ', 'အတွက်', 'သင်ကြား', 'သည်', '။']\n",
      "Token IDs (with BPE, text2): [26, 11, 13, 12, 10, 4, 21]\n"
     ]
    }
   ],
   "source": [
    "class BurmeseTokenizer:\n",
    "    def __init__(self, dictionary: Dict[str, str], token_freqs: Dict[str, float] = None, bpe_vocab_size: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize the Burmese tokenizer with a root-and-particle dictionary and optional token frequencies.\n",
    "        \n",
    "        Args:\n",
    "            dictionary (Dict[str, str]): Dictionary mapping words to 'root' or 'particle'.\n",
    "            token_freqs (Dict[str, float]): Token frequencies for disambiguation (optional).\n",
    "            bpe_vocab_size (int): Vocabulary size for BPE training.\n",
    "        \"\"\"\n",
    "        self.dictionary = dictionary\n",
    "        self.bpe_tokenizer = None\n",
    "        self.bpe_vocab_size = bpe_vocab_size\n",
    "        # Initialize trie with dictionary\n",
    "        self.trie = Trie()\n",
    "        for word in dictionary:\n",
    "            freq = token_freqs.get(word, 1.0) if token_freqs else 1.0\n",
    "            self.trie.insert(word, freq)\n",
    "\n",
    "    def segment_syllables(self, text: str) -> List[str]:\n",
    "        \"\"\"Segment Burmese text into syllables using syllable_split.\"\"\"\n",
    "        return syllable_split(text)\n",
    "    \n",
    "    def maximum_matching(self, syllables: List[str], beam_size: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Recombine syllables into root words and particles using probability-based maximum matching.\n",
    "        \n",
    "        Args:\n",
    "            syllables (List[str]): List of syllables.\n",
    "            beam_size (int): Number of segmentations to consider (for disambiguation).\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of tokenized root words and particles.\n",
    "        \"\"\"\n",
    "        # Beam search state: (position, tokens, score)\n",
    "        beam = [(0, [], 0.0)]  # Start with position 0, empty tokens, and score 0.0\n",
    "        final_segmentations = []\n",
    "\n",
    "        while beam:\n",
    "            new_beam = []\n",
    "            for pos, tokens, score in beam:\n",
    "                if pos >= len(syllables):\n",
    "                    # If we reach the end of the syllables, add the segmentation to final results\n",
    "                    final_segmentations.append((tokens, score))\n",
    "                    continue\n",
    "\n",
    "                # Find all possible matches at the current position\n",
    "                matches = self.trie.find_all_matches(syllables, pos)\n",
    "                # print(f\"Position: {pos}, Matches: {matches}\")\n",
    "                if not matches:\n",
    "                    # No match: treat as single syllable with low frequency\n",
    "                    matches = [(syllables[pos], 0.01, pos + 1)]  # Assign a very low frequency for unmatched syllables\n",
    "\n",
    "                # Extend the beam with each match\n",
    "                for token, freq, next_pos in matches:\n",
    "                    new_tokens = tokens + [token]\n",
    "                    # Score: log-sum of frequencies with a penalty for short tokens\n",
    "                    new_score = score + log(max(freq, 1e-10)) - 0.01 * len(token)\n",
    "                    new_beam.append((next_pos, new_tokens, new_score))\n",
    "\n",
    "            # Keep the top-k segmentations based on score\n",
    "            beam = heapq.nlargest(beam_size, new_beam, key=lambda x: x[2])\n",
    "\n",
    "        # Select the best segmentation from the final results\n",
    "        if final_segmentations:\n",
    "            best_tokens, _ = max(final_segmentations, key=lambda x: x[1])\n",
    "            return best_tokens\n",
    "\n",
    "        # Fallback: return syllables as-is if no valid segmentation is found\n",
    "        return syllables\n",
    "\n",
    "    def train_bpe(self, texts: List[str], special_tokens: List[str] = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]):\n",
    "        \"\"\"\n",
    "        Train a BPE tokenizer with dictionary constraints on a raw Burmese text corpus.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of raw Burmese texts (e.g., sentences from your dataset).\n",
    "            special_tokens (List[str]): Special tokens for NLP frameworks.\n",
    "        \"\"\"\n",
    "        self.bpe_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "        self.bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        self.bpe_tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "        # Pre-tokenize corpus using maximum_matching to protect dictionary tokens\n",
    "        pretokenized_texts = []\n",
    "        for text in texts:\n",
    "            syllables = self.segment_syllables(text)\n",
    "            tokens = self.maximum_matching(syllables)\n",
    "            pretokenized_texts.append(' '.join(tokens))\n",
    "\n",
    "        # Add dictionary tokens as protected tokens\n",
    "        protected_tokens = list(self.dictionary.keys()) + special_tokens\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=self.bpe_vocab_size,\n",
    "            special_tokens=protected_tokens,\n",
    "            initial_alphabet=[c for word in self.dictionary.keys() for c in word]\n",
    "        )\n",
    "        self.bpe_tokenizer.train_from_iterator(pretokenized_texts, trainer)\n",
    "        self.bpe_tokenizer.save(\"burmese_bpe_tokenizer.json\")\n",
    "    \n",
    "    def tokenize(self, text: str, use_bpe: bool = True) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Tokenize Burmese text into sub-word units.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input Burmese text.\n",
    "            use_bpe (bool): Whether to apply BPE after dictionary-based tokenization.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[List[str], List[int]]: List of tokens and their corresponding IDs.\n",
    "        \"\"\"\n",
    "        syllables = self.segment_syllables(text)\n",
    "        tokens = self.maximum_matching(syllables)\n",
    "        if use_bpe and self.bpe_tokenizer:\n",
    "            encoded = self.bpe_tokenizer.encode(' '.join(tokens))\n",
    "            return encoded.tokens, encoded.ids\n",
    "        return tokens, list(range(len(tokens)))\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Decode token IDs back to Burmese text.\"\"\"\n",
    "        if self.bpe_tokenizer:\n",
    "            return self.bpe_tokenizer.decode(token_ids)\n",
    "        return ''.join(self.dictionary.get(id, '[UNK]') for id in token_ids)\n",
    "\n",
    "    def batch_tokenize(self, texts: List[str], use_bpe: bool = True) -> List[Tuple[List[str], List[int]]]:\n",
    "        \"\"\"Tokenize a batch of texts for efficiency.\"\"\"\n",
    "        return [self.tokenize(text, use_bpe) for text in texts]\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    from math import log\n",
    "\n",
    "    # Example dictionary with frequencies\n",
    "    dictionary = {\n",
    "        \"ပညာရေး\": \"root\",\n",
    "        \"ပညာ\": \"root\",\n",
    "        \"ဝန်ကြီးဌာန\": \"root\",\n",
    "        \"ဝန်ကြီး\": \"root\",\n",
    "        \"သည်\": \"particle\",\n",
    "        \"အထက်တန်း\": \"root\",\n",
    "        \"အထက်\": \"root\",\n",
    "        \"ကျောင်း\": \"root\",\n",
    "        \"များ\": \"particle\",\n",
    "        \"တွင်\": \"particle\",\n",
    "        \"သင်ကြား\": \"root\",\n",
    "        \"ရေး\": \"particle\",\n",
    "        \"အတွက်\": \"particle\",\n",
    "        \"အထောက်အကူ\": \"root\",\n",
    "        \"ပြု\": \"root\",\n",
    "        \"စာအုပ်\": \"root\",\n",
    "        \"ကို\": \"particle\",\n",
    "        \"ထုတ်ဝေ\": \"root\",\n",
    "        \"ထုတ်\": \"root\",\n",
    "        \"ဝေ\": \"root\",\n",
    "        \"ခဲ့\": \"particle\",\n",
    "        \"။\": \"punctuation\",\n",
    "        \"၊\": \"punctuation\",\n",
    "        \"ဗိုလ်ချုပ်ကတော်\": \"root\",\n",
    "        \"ဗိုလ်ချုပ်\": \"root\",\n",
    "        \"က\": \"particle\",\n",
    "        \"တော်လှန်\": \"root\",\n",
    "        \"လှန်\": \"root\",\n",
    "        \"တော်\": \"root\",\n",
    "    }\n",
    "\n",
    "    # Example token frequencies (precomputed from your corpus)\n",
    "    token_freqs = {\n",
    "        \"ဗိုလ်ချုပ်ကတော်\": 0.01,  # Rare\n",
    "        \"ဗိုလ်ချုပ်\": 0.1,           # Common\n",
    "        \"က\": 0.5,                   # Very common\n",
    "        \"တော်လှန်\": 0.05,          # Moderately common\n",
    "        \"လှန်\": 0.02,              # Less common\n",
    "        \"တော်\": 0.03,             # Moderately common\n",
    "        \"သည်\": 0.4,               # Very common\n",
    "        # Add frequencies for other tokens\n",
    "    }\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BurmeseTokenizer(dictionary, token_freqs)\n",
    "\n",
    "    # Example texts\n",
    "    text1 = \"ပညာရေးဝန်ကြီးဌာနသည် အထက်တန်းကျောင်းများတွင် သင်ကြားမှုအတွက် သင်ကြားရေးအထောက်အကူပြု စာအုပ်များကို ထုတ်ဝေခဲ့သည်။\"\n",
    "    text2 = \"တော်လှန်ရေးအထောက်အကူအတွက်သင်ကြားသည်။\"\n",
    "    text3 = \"ဗိုလ်ချုပ်ကတော်လှန်သည်\"\n",
    "\n",
    "    # Tokenize without BPE\n",
    "    tokens_1, token_ids_1 = tokenizer.tokenize(text1, use_bpe=False)\n",
    "    print(\"Tokens (no BPE, text1):\", tokens_1)\n",
    "    print(\"Token IDs (no BPE, text1):\", token_ids_1)\n",
    "\n",
    "    # Tokenize without BPE\n",
    "    tokens_2, token_ids_2 = tokenizer.tokenize(text2, use_bpe=False)\n",
    "    print(\"Tokens (no BPE, text2):\", tokens_2)\n",
    "    print(\"Token IDs (no BPE, text2):\", token_ids_2)\n",
    "\n",
    "    # Train BPE on your raw Burmese text corpus\n",
    "    corpus = [text1, text2] * 50  # Replace with your actual raw text corpus\n",
    "    tokenizer.train_bpe(corpus)\n",
    "\n",
    "    # Tokenize with BPE\n",
    "    tokens_1_BPE, token_ids_1_BPE = tokenizer.tokenize(text1, use_bpe=True)\n",
    "    print(\"Tokens (with BPE, text1):\", tokens_1_BPE)\n",
    "    print(\"Token IDs (with BPE, text1):\", token_ids_1_BPE)\n",
    "\n",
    "    # Tokenize with BPE\n",
    "    tokens_2_BPE, token_ids_2_BPE = tokenizer.tokenize(text2, use_bpe=True)\n",
    "    print(\"Tokens (with BPE, text2):\", tokens_2_BPE)\n",
    "    print(\"Token IDs (with BPE, text2):\", token_ids_2_BPE)\n",
    "\n",
    "    # Decode example\n",
    "    # decoded_text = tokenizer.decode(token_ids)\n",
    "    # print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "    # Batch tokenize example\n",
    "    # batch_results = tokenizer.batch_tokenize([text1, text2], use_bpe=False)\n",
    "    # print(\"Batch tokens (no BPE):\", [tokens for tokens, _ in batch_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e414a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recombined tokens: ['ပညာရေး', 'ဝန်ကြီးဌာန', 'သည်']\n"
     ]
    }
   ],
   "source": [
    "# Example syllables\n",
    "syllables = [\"ပညာ\", \"ရေး\", \"ဝန်\", \"ကြီး\", \"ဌာန\", \"သည်\"]\n",
    "\n",
    "# Expected output: [\"ပညာရေး\", \"ဝန်ကြီးဌာန\", \"သည်\"]\n",
    "tokens = tokenizer.maximum_matching(syllables)\n",
    "print(\"Recombined tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a18529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches at position 0: [('ဗိုလ်ချုပ်', 0.1, 2), ('ဗိုလ်ချုပ်ကတော်', 0.01, 4)]\n",
      "Matches at position 1: []\n",
      "Matches at position 2: [('က', 0.5, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Example syllables\n",
    "syllables = [\"ဗိုလ်\", \"ချုပ်\", \"က\", \"တော်\", \"လှန်\", \"သည်\"]\n",
    "\n",
    "# Expected matches for each position\n",
    "print(\"Matches at position 0:\", tokenizer.trie.find_all_matches(syllables, 0))\n",
    "print(\"Matches at position 1:\", tokenizer.trie.find_all_matches(syllables, 1))\n",
    "print(\"Matches at position 2:\", tokenizer.trie.find_all_matches(syllables, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57e6374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllables: ['တော်', 'လှန်', 'ရေး', 'အ', 'ထောက်', 'အ', 'ကူ', 'အ', 'တွက်', 'သင်', 'ကြား', 'သည်', '။']\n"
     ]
    }
   ],
   "source": [
    "syllables = tokenizer.segment_syllables(text2)\n",
    "print(\"Syllables:\", syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9fd3295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches at position 0: [('ဗိုလ်ချုပ်', 0.1, 2)]\n",
      "Matches at position 1: []\n",
      "Matches at position 2: [('က', 0.5, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Example dictionary\n",
    "dictionary = {\n",
    "    \"ဗိုလ်ချုပ်\": 0.1,\n",
    "    \"က\": 0.5,\n",
    "    \"တော်\": 0.03,\n",
    "    \"လှန်\": 0.02,\n",
    "    \"သည်\": 0.4,\n",
    "}\n",
    "\n",
    "# Initialize Trie\n",
    "trie = Trie()\n",
    "for word, freq in dictionary.items():\n",
    "    trie.insert(word, freq)\n",
    "\n",
    "# Example syllables\n",
    "syllables = [\"ဗိုလ်\", \"ချုပ်\", \"က\", \"တော်\", \"လှန်\", \"သည်\"]\n",
    "\n",
    "# Test find_all_matches\n",
    "print(\"Matches at position 0:\", trie.find_all_matches(syllables, 0))\n",
    "print(\"Matches at position 1:\", trie.find_all_matches(syllables, 1))\n",
    "print(\"Matches at position 2:\", trie.find_all_matches(syllables, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
