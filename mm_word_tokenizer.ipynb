{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4070b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from typing import List, Dict, Tuple\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbfdce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syllable(label: str, burmese_consonant: str, others: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Segment a Burmese word into syllables using regex-based rules.\n",
    "    \n",
    "    Args:\n",
    "        label (str): Input Burmese text or word.\n",
    "        burmese_consonant (str): Regex range for Burmese consonants.\n",
    "        others (str): Regex range for other characters (vowels, punctuation, etc.).\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of syllables.\n",
    "    \"\"\"\n",
    "    # Define regex patterns for Burmese consonants and other characters\n",
    "    # label = re.sub(r\"(?<![္])([\"+burmese_consonant+\"])(?![်္|့])|([\"+others+\"])\", r\" \\1\\2\", label).strip()\n",
    "    # label = re.sub('(?<=[က-ၴ])([a-zA-Z0-9])', r' \\1', label)\n",
    "    # label = re.sub('([0-9၀-၉])\\s+([0-9၀-၉])\\s*', r'\\1\\2 ', label)\n",
    "    # label = re.sub('([0-9၀-၉])\\s+(\\+)', r'\\1 \\2 ', label)\n",
    "    # label = label.split()\n",
    "    label = re.sub(r\"(?<![္])([\" + burmese_consonant + r\"])(?![်္|့])|([\" + others + r\"])\", r\" \\1\\2\", label).strip()\n",
    "    label = re.sub(r\"(?<=[က-ၴ])([a-zA-Z0-9])\", r\" \\1\", label)\n",
    "    label = re.sub(r\"([0-9၀-၉])\\s+([0-9၀-၉])\\s*\", r\"\\1\\2 \", label)\n",
    "    label = re.sub(r\"([0-9၀-၉])\\s+(\\+)\", r\"\\1 \\2 \", label)\n",
    "    label = label.split()\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eff22d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_split(label: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split Burmese text into syllables, handling spaces and word boundaries.\n",
    "    \n",
    "    Args:\n",
    "        label (str): Input Burmese text.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of syllables.\n",
    "    \"\"\"\n",
    "    burmese_consonant = 'က-အ'\n",
    "    others = r\"ဣဤဥဦဧဩဪဿ၌၍၏၀-၉၊။!-/:-@[-`{-~\\s.,\"\n",
    "    \n",
    "    label_syllable = [get_syllable(s, burmese_consonant, others) + [' '] for s in label.split()]\n",
    "    return [s for sublist in label_syllable for s in sublist][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4348cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (no BPE): ['ပညာရေး', 'ဝန်ကြီးဌာန', 'သည်', ' ', 'အထက်တန်း', 'ကျောင်း', 'များ', 'တွင်', ' ', 'သင်ကြား', 'မှု', 'အတွက်', ' ', 'သင်ကြား', 'ရေး', 'အထောက်အကူ', 'ပြု', ' ', 'စာအုပ်', 'များ', 'ကို', ' ', 'ထုတ်ဝေ', 'ခဲ့', 'သည်', '။']\n",
      "Token IDs (no BPE): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "Tokens (with BPE): ['ပညာ', 'ရေး', 'ဝန်', 'ကြီး', 'ဌာ', 'န', 'သည်', 'အထက်တန်', 'း', 'ကျ', 'ော', 'င်', 'း', 'များ', 'တွင်', 'သင်ကြား', 'မှ', 'ု', 'အတွ', 'က်', 'သင်ကြား', 'ရေး', 'အထော', 'က်အကူ', 'ပြု', 'စာအု', 'ပ်', 'များ', 'ကိ', 'ု', 'ထုတ်ဝေ', 'ခဲ', '့', 'သည်', '။']\n",
      "Token IDs (with BPE): [66, 51, 69, 81, 59, 13, 46, 84, 28, 55, 48, 36, 28, 50, 82, 52, 67, 23, 71, 38, 52, 51, 83, 80, 97, 92, 64, 50, 53, 23, 95, 56, 27, 46, 35]\n",
      "Decoded text: ပညာ ရေး ဝန် ကြီး ဌာ န သည် အထက်တန် း ကျ ော င် း များ တွင် သင်ကြား မှ ု အတွ က် သင်ကြား ရေး အထော က်အကူ ပြု စာအု ပ် များ ကိ ု ထုတ်ဝေ ခဲ ့ သည် ။\n"
     ]
    }
   ],
   "source": [
    "class BurmeseTokenizer:\n",
    "    def __init__(self, dictionary: Dict[str, str], bpe_vocab_size: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize the Burmese tokenizer with a root-and-particle dictionary.\n",
    "        \n",
    "        Args:\n",
    "            dictionary (Dict[str, str]): Dictionary mapping words to 'root' or 'particle'.\n",
    "            bpe_vocab_size (int): Vocabulary size for BPE training.\n",
    "        \"\"\"\n",
    "        self.dictionary = dictionary\n",
    "        self.bpe_tokenizer = None\n",
    "        self.bpe_vocab_size = bpe_vocab_size\n",
    "    \n",
    "    def segment_syllables(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Segment Burmese text into syllables using the provided syllable_split function.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input Burmese text.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of syllables.\n",
    "        \"\"\"\n",
    "        return syllable_split(text)\n",
    "    \n",
    "    def maximum_matching(self, syllables: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Recombine syllables into root words and particles using maximum matching.\n",
    "        \n",
    "        Args:\n",
    "            syllables (List[str]): List of syllables.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of tokenized root words and particles.\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        max_len = max(len(word) for word in self.dictionary)  # Max word length in dictionary\n",
    "        \n",
    "        while i < len(syllables):\n",
    "            matched = False\n",
    "            # Try matching longest possible word from current position\n",
    "            for length in range(max_len, 0, -1):\n",
    "                if i + length <= len(syllables):\n",
    "                    candidate = ''.join(syllables[i:i + length])\n",
    "                    if candidate in self.dictionary:\n",
    "                        tokens.append(candidate)\n",
    "                        i += length\n",
    "                        matched = True\n",
    "                        break\n",
    "            if not matched:\n",
    "                # If no match, treat as single syllable\n",
    "                tokens.append(syllables[i])\n",
    "                i += 1\n",
    "        return tokens\n",
    "    \n",
    "    def train_bpe(self, texts: List[str], special_tokens: List[str] = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]):\n",
    "        \"\"\"\n",
    "        Train a BPE tokenizer with dictionary constraints on a corpus.\n",
    "        \n",
    "        Args:\n",
    "            texts (List[str]): List of Burmese texts for training.\n",
    "            special_tokens (List[str]): Special tokens for NLP frameworks.\n",
    "        \"\"\"\n",
    "        # Initialize BPE tokenizer\n",
    "        self.bpe_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "        \n",
    "        # Add dictionary words as pre-tokenized units\n",
    "        self.bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "        \n",
    "        # Train BPE\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=self.bpe_vocab_size,\n",
    "            special_tokens=special_tokens,\n",
    "            initial_alphabet=list(self.dictionary.keys())  # Ensure dictionary words are prioritized\n",
    "        )\n",
    "        self.bpe_tokenizer.train_from_iterator(texts, trainer)\n",
    "        \n",
    "        # Save tokenizer for reuse\n",
    "        self.bpe_tokenizer.save(\"burmese_bpe_tokenizer.json\")\n",
    "    \n",
    "    def tokenize(self, text: str, use_bpe: bool = True) -> Tuple[List[str], List[int]]:\n",
    "        \"\"\"\n",
    "        Tokenize Burmese text into sub-word units.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input Burmese text.\n",
    "            use_bpe (bool): Whether to apply BPE after dictionary-based tokenization.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[List[str], List[int]]: List of tokens and their corresponding IDs.\n",
    "        \"\"\"\n",
    "        # Step 1: Segment into syllables using custom syllable_split\n",
    "        syllables = self.segment_syllables(text)\n",
    "        \n",
    "        # Step 2: Apply maximum matching to get root words and particles\n",
    "        tokens = self.maximum_matching(syllables)\n",
    "        \n",
    "        if use_bpe and self.bpe_tokenizer:\n",
    "            # Step 3: Apply BPE for sub-word tokenization\n",
    "            encoded = self.bpe_tokenizer.encode(' '.join(tokens))\n",
    "            return encoded.tokens, encoded.ids\n",
    "        else:\n",
    "            # Return dictionary-based tokens only\n",
    "            return tokens, list(range(len(tokens)))  # Dummy IDs for non-BPE case\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token IDs back to Burmese text.\n",
    "        \n",
    "        Args:\n",
    "            token_ids (List[int]): List of token IDs.\n",
    "        \n",
    "        Returns:\n",
    "            str: Decoded Burmese text.\n",
    "        \"\"\"\n",
    "        if self.bpe_tokenizer:\n",
    "            return self.bpe_tokenizer.decode(token_ids)\n",
    "        return ''.join(self.dictionary.get(id, '[UNK]') for id in token_ids)\n",
    "    \n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dictionary (from your previous input)\n",
    "    dictionary = {\n",
    "        \"ပညာရေး\": \"root\",\n",
    "        \"ပညာ\": \"root\",\n",
    "        \"ဝန်ကြီးဌာန\": \"root\",\n",
    "        \"ဝန်ကြီး\": \"root\",\n",
    "        \"သည်\": \"particle\",\n",
    "        \"အထက်တန်း\": \"root\",\n",
    "        \"အထက်\": \"root\",\n",
    "        \"ကျောင်း\": \"root\",\n",
    "        \"များ\": \"particle\",\n",
    "        \"တွင်\": \"particle\",\n",
    "        \"သင်ကြား\": \"root\",\n",
    "        \"ရေး\": \"particle\",\n",
    "        \"အတွက်\": \"particle\",\n",
    "        \"အထောက်အကူ\": \"root\",\n",
    "        \"ပြု\": \"root\",\n",
    "        \"စာအုပ်\": \"root\",\n",
    "        \"ကို\": \"particle\",\n",
    "        \"ထုတ်ဝေ\": \"root\",\n",
    "        \"ထုတ်\": \"root\",\n",
    "        \"ဝေ\": \"root\",\n",
    "        \"ခဲ့\": \"particle\",\n",
    "        \"။\": \"punctuation\",\n",
    "        \"၊\": \"punctuation\",\n",
    "    }\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = BurmeseTokenizer(dictionary)\n",
    "\n",
    "    # Example text\n",
    "    text = \"ပညာရေးဝန်ကြီးဌာနသည် အထက်တန်းကျောင်းများတွင် သင်ကြားမှုအတွက် သင်ကြားရေးအထောက်အကူပြု စာအုပ်များကို ထုတ်ဝေခဲ့သည်။\"\n",
    "\n",
    "    # Tokenize without BPE\n",
    "    tokens, token_ids = tokenizer.tokenize(text, use_bpe=False)\n",
    "    print(\"Tokens (no BPE):\", tokens)\n",
    "    print(\"Token IDs (no BPE):\", token_ids)\n",
    "\n",
    "    # Train BPE on a dummy corpus (replace with your dataset)\n",
    "    corpus = [text] * 100  # Dummy corpus; use your actual dataset\n",
    "    tokenizer.train_bpe(corpus)\n",
    "\n",
    "    # Tokenize with BPE\n",
    "    tokens, token_ids = tokenizer.tokenize(text, use_bpe=True)\n",
    "    print(\"Tokens (with BPE):\", tokens)\n",
    "    print(\"Token IDs (with BPE):\", token_ids)\n",
    "\n",
    "    # Decode example\n",
    "    decoded_text = tokenizer.decode(token_ids)\n",
    "    print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ab82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain integration\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "hf_pipeline = pipeline(\"text-generation\", model=\"facebook/mbart-large-50\", tokenizer=tokenizer.bpe_tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "template = \"Answer in Burmese: {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "response = llm_chain.run(\"What is the law regarding contracts in Myanmar?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spelling(text: str, tokenizer: BurmeseTokenizer) -> List[Tuple[str, bool]]:\n",
    "    tokens, _ = tokenizer.tokenize(text, use_bpe=False)\n",
    "    return [(token, token in tokenizer.dictionary or token in {'[UNK]'}) for token in tokens]\n",
    "\n",
    "result = check_spelling(text, tokenizer)\n",
    "for token, is_valid in result:\n",
    "    print(f\"Token: {token}, Valid: {is_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9e0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace integration\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"burmese_bpe_tokenizer.json\")\n",
    "inputs = tokenizer_hf(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66955a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch integration/ TensorFlow integration\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"burmese_bpe_tokenizer.json\")\n",
    "inputs = tokenizer_hf(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizer():\n",
    "    tokenizer = BurmeseTokenizer(dictionary)\n",
    "    text = \"ပညာရေးဝန်ကြီးဌာနသည်\"\n",
    "    tokens, _ = tokenizer.tokenize(text, use_bpe=False)\n",
    "    assert tokens == ['ပညာရေး', 'ဝန်ကြီးဌာန', 'သည်'], \"Tokenization failed\"\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell check test\n",
    "text = \"ပညာရေးဝန်ကြီးဌာနသည်\"  # Add a misspelled word for testing\n",
    "result = check_spelling(text, tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling\n",
    "import logging\n",
    "if not matched: \n",
    "    logging.warning(f\"Unmapped syllable at position {i}: {syllables[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch tokenization\n",
    "def batch_tokenize(self, texts: List[str], use_bpe: bool = True) -> List[Tuple[List[str], List[int]]]:\n",
    "    return [self.tokenize(text, use_bpe) for text in texts]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
